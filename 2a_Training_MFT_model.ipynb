{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9aff10-0ce1-4b9e-8bf5-bbf4de7fc926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahaque2/venv/py3_7/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "import torch\n",
    "\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5e84cf-3b2a-4605-b2d4-e5b694da6435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/MFTC_full_dataset.csv')\n",
    "# data = pd.read_csv('dataset/MFTC_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f315109-1e39-41f8-be65-a9fb07886a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data['label_id'] = pd.factorize(data.label)[0]\n",
    "# data = data[((data.label_id != 11) & (data.label_id != -1))].dropna(subset = ['text'])\n",
    "# data.to_csv('dataset/MFTC_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3801879-c39c-4d48-a39d-250f9bcf17a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33679, 8),\n",
       " Index(['Unnamed: 0', 'Unnamed: 0.1', 'ids', 'text', 'top', 'label',\n",
       "        'clean_text', 'label_id'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716b986c-5cc0-4b76-aa54-80a38fa6ad4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non-moral      14867\n",
       "harm            3332\n",
       "cheating        2972\n",
       "care            2257\n",
       "fairness        2118\n",
       "loyalty         1875\n",
       "subversion      1700\n",
       "authority       1496\n",
       "degradation     1286\n",
       "betrayal        1119\n",
       "purity           657\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7e2471-c5e8-4b8a-a760-44740ef2e010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     14867\n",
       "3      3332\n",
       "2      2972\n",
       "0      2257\n",
       "5      2118\n",
       "8      1875\n",
       "1      1700\n",
       "7      1496\n",
       "10     1286\n",
       "6      1119\n",
       "9       657\n",
       "Name: label_id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ca130a-fa9c-48e7-8c47-3b908a3d7a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna(subset = ['clean_text'])\n",
    "data.clean_text.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f759cc-d685-4f90-be24-8da7e9cac0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \n",
    "    #Removes Numbers\n",
    "    data = data.astype(str).str.replace('\\d+', '')\n",
    "    lower_text = data.str.lower()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer =  TweetTokenizer()\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        return [(lemmatizer.lemmatize(w)) for w \\\n",
    "                       in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "    def preprocess(txt):\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        exclude_words = ['donald', 'trump', 'joe', 'biden', 'president']\n",
    "        return [w for w in txt if (not w.lower() in stop_words and not w.lower() in exclude_words)]\n",
    "\n",
    "        # # print(txt)\n",
    "        # # sys.exit()\n",
    "        # proc_sent = []\n",
    "        # # print(txt)\n",
    "        # # sys.exit()\n",
    "        # for line in txt:\n",
    "        #     # word_tokens = word_tokenize(line)\n",
    "        #     print(line)\n",
    "#             sent = [w for w in line if (not w.lower() in stop_words and not w.lower() in exclude_words)]\n",
    "#             # pd.concat((proc_sent, pd.Series(sent)), axis = 0)\n",
    "#             proc_sent.append(sent)\n",
    "#             print(proc_sent)\n",
    "#             sys.exit()\n",
    "\n",
    "#         return proc_sent\n",
    "    \n",
    "    def remove_punctuation(words):\n",
    "        new_words = []\n",
    "        # print(words)\n",
    "        # sys.exit()\n",
    "        for word in words:\n",
    "          new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "          if new_word != '':\n",
    "             new_words.append(new_word)\n",
    "        # print(new_words)\n",
    "        # sys.exit()\n",
    "        \n",
    "        return new_words\n",
    "    \n",
    "    words = lower_text.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    words = words.apply(preprocess)\n",
    "    \n",
    "    # print(words)\n",
    "    # sys.exit()\n",
    "    \n",
    "    return pd.DataFrame(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a667221d-ebf7-44a3-8c39-d3c849855422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33419, 8),\n",
       " Index(['Unnamed: 0', 'Unnamed: 0.1', 'ids', 'text', 'top', 'label',\n",
       "        'clean_text', 'label_id'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c9a08ac-c14f-45d7-91a8-0bd8d1bd5d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data[['label', 'label_id']].drop_duplicates().to_csv('dataset/MFT_label_to_id_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13f4f89-6fb8-4a28-b6ff-02d49be55dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(data.text)\n",
    "y = np.array(data.label_id)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.text, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce9f2a44-31e9-4f37-8100-7cae1827bec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.dropna(subset = ['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a68ffd-e711-4df7-a919-be6879e5eb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b442ddf-8e06-4625-b04a-577e2c6f8762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.index = list(range(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "173882a2-5b9b-497e-89c6-5ef8e02dda1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data.index = list(range(data.shape[0]))\n",
    "# # data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb41dd6-0aa5-43b3-9989-73d637cab4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f462663-9ec6-4017-9168-5bf935d1a436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base', use_fast=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', use_fast=True)\n",
    "\n",
    "# model = AutoModel.from_pretrained('vinai/bertweet-base', num_labels=11)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=11)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base', num_labels=11)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82415b26-10ab-4fd0-95f2-56883392226f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_encodings = tokenizer(X_train.tolist(), truncation=True, max_length=500, padding=True)\n",
    "# test_encodings = tokenizer(X_test.tolist(), truncation=True, max_length=500, padding=True)\n",
    "\n",
    "data_encodings = tokenizer(data.clean_text.tolist(), truncation=True, max_length=130, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d773ea18-fb0a-4006-9e20-48b2c2e515f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "065e3877-9c7f-4e47-a401-cac6b55054c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33419,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data.clean_text.tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a134f76-cbe9-44da-b59a-55732dee4821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9617b812-9231-4e92-ab06-bdc4b0c9155c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class setDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# train_dataset = setDataset(train_encodings, y_train)\n",
    "# test_dataset = setDataset(test_encodings, y_test)\n",
    "\n",
    "dataset = setDataset(data_encodings, data.label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ba1094-1dab-4813-b856-8a879dbd7c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "def get_pytorch_model(model, train_dataset):\n",
    "\n",
    "    device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # device = torch.device('cpu')\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for batch in train_loader:\n",
    "            optim.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # print(input_ids.size(), attention_mask.shape, labels.shape)\n",
    "            # sys.exit()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a66b2b09-b631-402b-ab9e-9949e86555d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_pytorch_model(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eff2799-904b-4c24-951d-9d7c5d8200e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0df6c42f-933a-4850-8756-fcec9c04620d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# model.save_pretrained(\"model/model_MFT_all\")\n",
    "# pickle.dump(topics, open( \"bertopic/topics.pickle\", \"wb\" ) )\n",
    "# assert topics == new_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ae2e2-82d5-4473-bb5f-9e64920c9a38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base', use_fast=True)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"model/model_MFT_all\", num_labels=11).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88f5e730-f640-4eff-9e92-09b6aadbd959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca7a0369-3d42-4964-957c-9e60007796d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(X_test, tokenizer, model):\n",
    "    \n",
    "    x = 0\n",
    "    pred_labs = []\n",
    "    while x < X_test.shape[0]:\n",
    "        #model = DistilBertForSequenceClassification.from_pretrained(\"models/DistilBert_1\", num_labels=4).to(device)\n",
    "        device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        # y = x + 100\n",
    "        if((x+10) < X_test.shape[0]):\n",
    "            y = x + 10\n",
    "        else:\n",
    "            y = X_test.shape[0]\n",
    "\n",
    "        x_test = X_test.iloc[x:y].tolist()\n",
    "        # print(x, y, len(x_test))\n",
    "        inputs = tokenizer(x_test,  padding=True, truncation=True, max_length=500, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        #softmax_labs = [(round(x.tolist()[0], 4), round(x.tolist()[1], 4)) for x in outputs[0].softmax(1)]\n",
    "        y_pred = [int(x.argmax()) for x in outputs[0].softmax(1)]\n",
    "        pred_labs.extend(y_pred)\n",
    "        x+=10\n",
    "        \n",
    "    return pred_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adecbe84-de24-4937-b58b-0e3b0e4b8e02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6684"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = get_predictions(X_test, tokenizer, model)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eabaf12e-3260-419a-992e-661c3ef66587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.73       451\n",
      "           1       0.58      0.43      0.49       339\n",
      "           2       0.54      0.82      0.65       594\n",
      "           3       0.70      0.71      0.71       666\n",
      "           4       0.87      0.83      0.85      2927\n",
      "           5       0.72      0.74      0.73       423\n",
      "           6       0.47      0.33      0.39       224\n",
      "           7       0.60      0.68      0.64       299\n",
      "           8       0.86      0.55      0.67       373\n",
      "           9       0.54      0.74      0.62       131\n",
      "          10       0.55      0.62      0.58       257\n",
      "\n",
      "    accuracy                           0.73      6684\n",
      "   macro avg       0.66      0.65      0.64      6684\n",
      "weighted avg       0.75      0.73      0.74      6684\n",
      "\n",
      "[[ 311    2   10   31   43    9    2   17    6    8   12]\n",
      " [   1  146   59   16   47    4   33   16    0    0   17]\n",
      " [   0    8  488   23   36   14   11    3    0    0   11]\n",
      " [  10    7   68  475   60    3    7    5    0    2   29]\n",
      " [  35   39  130   72 2437   61   10   27   16   56   44]\n",
      " [   6    1   47    7   20  315    2   11    3    8    3]\n",
      " [   0   10   63   32   33    2   75    4    0    0    5]\n",
      " [   2   25    7    6   37    4    1  204    6    5    2]\n",
      " [  27    3    2    6   56   24    4   44  204    3    0]\n",
      " [   7    0    1    3    5    0    1    8    1   97    8]\n",
      " [   0   12   29    9   31    0   14    0    0    2  160]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97879a65-c210-41cf-bb59-005536b9f04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_7",
   "language": "python",
   "name": "py3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
